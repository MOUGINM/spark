{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33d4535e-9db8-4cd7-a9ef-1f6365fdaad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Créer une session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Mushroom Classification\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fcce981-84ca-4bd5-a46d-53269efa2df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------+----------+--------+-----+----------------+-------------+----------+-----------+------------+-----------+-------------------------+-------------------------+-----------------------+-----------------------+----------+-----------+------------+----------+------------------+-----------+-------+\n",
      "|class |cap-shape |cap-surface |cap-color |bruises |odor |gill-attachment |gill-spacing |gill-size |gill-color |stalk-shape |stalk-root |stalk-surface-above-ring |stalk-surface-below-ring |stalk-color-above-ring |stalk-color-below-ring |veil-type |veil-color |ring-number |ring-type |spore-print-color |population |habitat|\n",
      "+------+----------+------------+----------+--------+-----+----------------+-------------+----------+-----------+------------+-----------+-------------------------+-------------------------+-----------------------+-----------------------+----------+-----------+------------+----------+------------------+-----------+-------+\n",
      "|p     |x         |s           |n         |t       |p    |f               |c            |n         |k          |e           |e          |     s                ...|     s                ...|   w                ...|   w                ...|p         |w          |o           |p         |k                 |s          |      u|\n",
      "|e     |x         |s           |y         |t       |a    |f               |c            |b         |k          |e           |c          |     s                ...|     s                ...|   w                ...|   w                ...|p         |w          |o           |p         |n                 |n          |      g|\n",
      "|e     |b         |s           |w         |t       |l    |f               |c            |b         |n          |e           |c          |     s                ...|     s                ...|   w                ...|   w                ...|p         |w          |o           |p         |n                 |n          |      m|\n",
      "|p     |x         |y           |w         |t       |p    |f               |c            |n         |n          |e           |e          |     s                ...|     s                ...|   w                ...|   w                ...|p         |w          |o           |p         |k                 |s          |      u|\n",
      "|e     |x         |s           |g         |f       |n    |f               |w            |b         |k          |t           |e          |     s                ...|     s                ...|   w                ...|   w                ...|p         |w          |o           |e         |n                 |a          |      g|\n",
      "+------+----------+------------+----------+--------+-----+----------------+-------------+----------+-----------+------------+-----------+-------------------------+-------------------------+-----------------------+-----------------------+----------+-----------+------------+----------+------------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remplace le chemin par celui du fichier CSV\n",
    "data = spark.read.csv(\"mushrooms.csv\", header=True, inferSchema=True)\n",
    "data.show(5)  # Afficher les 5 premières lignes pour vérifier le chargement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cca83dbf-266d-42e6-8088-a2ca76a1cd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| class|label|\n",
      "+------+-----+\n",
      "|p     |  1.0|\n",
      "|e     |  0.0|\n",
      "|e     |  0.0|\n",
      "|p     |  1.0|\n",
      "|e     |  0.0|\n",
      "+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "# Retire l'espace de fin pour chaque nom de colonne\n",
    "for col_name in data.columns:\n",
    "    data = data.withColumnRenamed(col_name, col_name.strip())\n",
    "\n",
    "# Encode la colonne 'class' en numérique pour créer la colonne 'label'\n",
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "data = indexer.fit(data).transform(data)\n",
    "data.select(\"class\", \"label\").show(5)  # Vérifie l'encodage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a97bb6a0-794d-452e-837f-f38c68f517ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+---------------+-------------+----------+---------------------+------------------+---------------+----------------+-----------------+----------------+------------------------------+------------------------------+----------------------------+----------------------------+---------------+----------------+-----------------+---------------+-----------------------+----------------+-------------+-----+\n",
      "|cap-shape_index|cap-surface_index|cap-color_index|bruises_index|odor_index|gill-attachment_index|gill-spacing_index|gill-size_index|gill-color_index|stalk-shape_index|stalk-root_index|stalk-surface-above-ring_index|stalk-surface-below-ring_index|stalk-color-above-ring_index|stalk-color-below-ring_index|veil-type_index|veil-color_index|ring-number_index|ring-type_index|spore-print-color_index|population_index|habitat_index|label|\n",
      "+---------------+-----------------+---------------+-------------+----------+---------------------+------------------+---------------+----------------+-----------------+----------------+------------------------------+------------------------------+----------------------------+----------------------------+---------------+----------------+-----------------+---------------+-----------------------+----------------+-------------+-----+\n",
      "|            0.0|              1.0|            0.0|          1.0|       6.0|                  0.0|               0.0|            1.0|             7.0|              1.0|             2.0|                           0.0|                           0.0|                         0.0|                         0.0|            0.0|             0.0|              0.0|            0.0|                    2.0|             2.0|          4.0|  1.0|\n",
      "|            0.0|              1.0|            3.0|          1.0|       4.0|                  0.0|               0.0|            0.0|             7.0|              1.0|             3.0|                           0.0|                           0.0|                         0.0|                         0.0|            0.0|             0.0|              0.0|            0.0|                    1.0|             3.0|          1.0|  0.0|\n",
      "|            3.0|              1.0|            4.0|          1.0|       5.0|                  0.0|               0.0|            0.0|             3.0|              1.0|             3.0|                           0.0|                           0.0|                         0.0|                         0.0|            0.0|             0.0|              0.0|            0.0|                    1.0|             3.0|          5.0|  0.0|\n",
      "|            0.0|              0.0|            4.0|          1.0|       6.0|                  0.0|               0.0|            1.0|             3.0|              1.0|             2.0|                           0.0|                           0.0|                         0.0|                         0.0|            0.0|             0.0|              0.0|            0.0|                    2.0|             2.0|          4.0|  1.0|\n",
      "|            0.0|              1.0|            1.0|          0.0|       0.0|                  0.0|               1.0|            0.0|             7.0|              0.0|             2.0|                           0.0|                           0.0|                         0.0|                         0.0|            0.0|             0.0|              0.0|            1.0|                    1.0|             4.0|          1.0|  0.0|\n",
      "+---------------+-----------------+---------------+-------------+----------+---------------------+------------------+---------------+----------------+-----------------+----------------+------------------------------+------------------------------+----------------------------+----------------------------+---------------+----------------+-----------------+---------------+-----------------------+----------------+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Liste des colonnes catégorielles à encoder (toutes sauf \"label\" et \"features\")\n",
    "categorical_columns = [col for col in data.columns if col not in [\"class\", \"label\"]]\n",
    "\n",
    "# Appliquer StringIndexer sur chaque colonne catégorielle\n",
    "for col_name in categorical_columns:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_index\")\n",
    "    data = indexer.fit(data).transform(data)\n",
    "\n",
    "# Vérifie l’encodage\n",
    "data.select([f\"{col}_index\" for col in categorical_columns] + [\"label\"]).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67cdf076-4502-4f51-85ac-bcefa480e18d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(22,[1,3,4,7,8,9,...|  1.0|\n",
      "|(22,[1,2,3,4,8,9,...|  0.0|\n",
      "|(22,[0,1,2,3,4,8,...|  0.0|\n",
      "|(22,[2,3,4,7,8,9,...|  1.0|\n",
      "|(22,[1,2,6,8,10,1...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Utiliser les colonnes encodées dans VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[f\"{col}_index\" for col in categorical_columns], outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "data.select(\"features\", \"label\").show(5)  # Vérifie l'assemblage des caractéristiques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55c13528-37be-4cd7-b1f5-80af68bff69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11056f6c-6a30-4874-a126-cd23d3a229db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialiser et entraîner le modèle de régression logistique\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = lr.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1166a8c4-5b8a-47e8-b04d-44a10dd7aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions sur les données de test\n",
    "predictions = model.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05b7cded-608d-410a-b92d-3cf968851e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Précision\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"precisionByLabel\")\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "\n",
    "# Rappel\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"recallByLabel\")\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "\n",
    "# F1-score\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = evaluator_f1.evaluate(predictions)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65230346-62f9-4f07-86ad-df863632c761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|  748|\n",
      "|  0.0|       1.0|    6|\n",
      "|  1.0|       0.0|   15|\n",
      "|  0.0|       0.0|  782|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tableau de contingence pour observer FP et VP\n",
    "predictions.groupBy(\"label\", \"prediction\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cea5866-64df-4e02-a98a-94c6c7cb2467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision : 0.98\n",
      "Rappel : 0.99\n",
      "F1-score : 0.99\n",
      "AUC de la courbe ROC : 1.00\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# AUC pour la courbe ROC\n",
    "evaluator_roc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator_roc.evaluate(predictions)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# AUC pour la courbe ROC\n",
    "evaluator_roc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator_roc.evaluate(predictions)\n",
    "\n",
    "print(f\"Précision : {precision:.2f}\")\n",
    "print(f\"Rappel : {recall:.2f}\")\n",
    "print(f\"F1-score : {f1_score:.2f}\")\n",
    "print(f\"AUC de la courbe ROC : {auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "372dedcc-7a78-403b-94d1-bffc6c7b0e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|  748|\n",
      "|  0.0|       1.0|    6|\n",
      "|  1.0|       0.0|   15|\n",
      "|  0.0|       0.0|  782|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tableau de contingence pour observer FP et VP\n",
    "predictions.groupBy(\"label\", \"prediction\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "778dbfdf-5421-48a7-a5fc-504f36cb844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mushroom_classification_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d11a43f-bbd4-4602-ba1f-e3ba0a3495e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Flask\n",
      "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting Werkzeug>=3.0.0 (from Flask)\n",
      "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /opt/conda/lib/python3.11/site-packages (from Flask) (3.1.2)\n",
      "Collecting itsdangerous>=2.1.2 (from Flask)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: click>=8.1.3 in /opt/conda/lib/python3.11/site-packages (from Flask) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.11/site-packages (from Flask) (1.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from Jinja2>=3.1.2->Flask) (2.1.3)\n",
      "Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Werkzeug, itsdangerous, Flask\n",
      "Successfully installed Flask-3.0.3 Werkzeug-3.0.6 itsdangerous-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install Flask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13e6cca7-a5ee-42d0-98cc-de1218f75746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API démarrée sur http://localhost:5000\n"
     ]
    }
   ],
   "source": [
    "# Importer les bibliothèques\n",
    "from flask import Flask, request, jsonify\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import json\n",
    "from werkzeug.serving import make_server\n",
    "import threading\n",
    "\n",
    "# Charger le modèle PySpark\n",
    "model = LogisticRegressionModel.load(\"mushroom_classification_model\")\n",
    "\n",
    "# Initialiser l’application Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Créer le point d’entrée pour les prédictions\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        # Recevoir les données JSON du champignon à classifier\n",
    "        data = request.json\n",
    "        # Convertir les caractéristiques en vecteur\n",
    "        features = Vectors.dense(data[\"features\"])\n",
    "        \n",
    "        # Faire la prédiction\n",
    "        prediction = model.predict(features)\n",
    "        \n",
    "        # Retourner la prédiction\n",
    "        return jsonify({\"prediction\": int(prediction)})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)})\n",
    "\n",
    "# Lancer le serveur Flask dans un thread séparé pour le maintenir actif dans Jupyter\n",
    "class FlaskThread(threading.Thread):\n",
    "    def __init__(self, app):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.server = make_server('0.0.0.0', 5000, app)\n",
    "        self.ctx = app.app_context()\n",
    "        self.ctx.push()\n",
    "\n",
    "    def run(self):\n",
    "        print(\"API démarrée sur http://localhost:5000\")\n",
    "        self.server.serve_forever()\n",
    "\n",
    "    def shutdown(self):\n",
    "        self.server.shutdown()\n",
    "\n",
    "# Créer et démarrer le thread Flask\n",
    "flask_thread = FlaskThread(app)\n",
    "flask_thread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d086004-8478-4fb8-9312-e44ea968120b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [30/Oct/2024 09:36:04] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 1}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Exemple de caractéristiques (remplace par des valeurs réelles du champignon)\n",
    "features = [0.0, 1.0, 3.0, 1.0, 4.0, 0.0, 0.0, 1.0, 7.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 4.0]\n",
    "\n",
    "response = requests.post(\"http://localhost:5000/predict\", json={\"features\": features})\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e503b-dd40-434c-9166-9a0432ef8939",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "0 : Champignon comestible.\n",
    "1 : Champignon toxique.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18431ea4-c97b-470c-a556-e5041fd76da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
